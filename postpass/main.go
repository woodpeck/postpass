/*
 * "postpass"
 *
 * a simple wrapper around PostGIS that allows random people on the
 * internet to run PostGIS queries without ruining everything
 *
 * written by Frederik Ramm, GPL3+
 */

package main

import (
	"database/sql"
	"fmt"
	_ "github.com/lib/pq"
	"log"
	"net/http"
	"regexp"
	"strconv"
	"strings"
	"sync"
	"time"
)

/* config stuff
 * should go into commandline arguments
 */
const (
	Host                 = "localhost"
	Port                 = 5432
	User                 = "readonly"
	Password             = "readonly"
	DBName               = "gis"
	QuickMediumThreshold = 150
	MediumSlowThreshold  = 150000
	ListenPort           = 8081
)

// SqlResponse encapsulates a response generated by a worker
type SqlResponse struct {
	err    bool
	result string
}

// WorkItem encapsulates a job sent to a worker
type WorkItem struct {
	request    string
	geojson    bool
	pretty     bool
	collection bool
	response   chan SqlResponse
}

// global request counter
var countMutex sync.Mutex
var count = 0

// global counter for idle workers
var idleMutex sync.Mutex
var idle [4]int

/*
 * worker function that executes SQL queries
 *
 * arguments: database connection, worker id, channel to read jobs from
 */
func worker(db *sql.DB, id int, tasks <-chan WorkItem) {
	var res string
	idleMutex.Lock()
	idle[id/100]++
	idleMutex.Unlock()

	// reads job from channel
	for task := range tasks {
		// log.Printf("worker %d processing task '%s'\n", id, task.request)
		idleMutex.Lock()
		idle[id/100]--
		idleMutex.Unlock()

		// this executes the request on the database.
		var rows *sql.Rows
		var err error

		if !task.collection {

			// if task.collection is not set, we execute the query as-is.
			// this will only work if the query returns exactly one row and one column.
			rows, err = db.Query(task.request)

		} else if task.geojson && task.pretty {

			// this generates prettified GeoJSON

			rows, err = db.Query(fmt.Sprintf(
				`SELECT jsonb_pretty(jsonb_build_object(
                    'type', 'FeatureCollection',
                    'properties', jsonb_build_object(
                       'timestamp', (select value from osm2pgsql_properties where property='replication_timestamp'),
                       'generator', 'Postpass API 0.1'
                       ),
                    'features', coalesce(jsonb_agg(ST_AsGeoJSON(t.*)::json), '[]'::jsonb)))
                FROM (%s) as t;`, task.request))

		} else if task.geojson && !task.pretty {

			// this generates un-prettified GeoJSON

			rows, err = db.Query(fmt.Sprintf(
				`SELECT json_build_object(
                    'type', 'FeatureCollection',
                    'properties', jsonb_build_object(
                       'timestamp', (select value from osm2pgsql_properties where property='replication_timestamp'),
                       'generator', 'Postpass API 0.1'
                       ),
                    'features', coalesce(jsonb_agg(ST_AsGeoJSON(t.*)::json), '[]'::jsonb))
                FROM (%s) as t;`, task.request))

		} else {

			// this collects results over multiple rows and columns,
			// but doesn't attempt to build GeoJSON

			rows, err = db.Query(fmt.Sprintf(
				`SELECT jsonb_pretty(jsonb_build_object(
                    'metadata', jsonb_build_object(
                       'timestamp', (select value from osm2pgsql_properties where property='replication_timestamp'),
                       'generator', 'Postpass API 0.1'
                       ),
                    'result', jsonb_agg(t.*)::jsonb))
                FROM (%s) as t;`, task.request))
		}

		if err != nil {
			task.response <- SqlResponse{err: true, result: err.Error()}
			idleMutex.Lock()
			idle[id/100]++
			idleMutex.Unlock()
			continue
		}

		// parse only one line of results
		rows.Next()

		// scan only one column of the result line
		err = rows.Scan(&res)

		// discard result
		rows.Close()

		if err != nil {
			task.response <- SqlResponse{err: true, result: err.Error()}
			idleMutex.Lock()
			idle[id/100]++
			idleMutex.Unlock()
			continue
		}

		// log.Printf("worker %d done\n", id)

		// send response back on channel
		task.response <- SqlResponse{err: false, result: res}
		idleMutex.Lock()
		idle[id/100]++
		idleMutex.Unlock()
	}
}

/*
 * API handler that receives a web request
 *
 * executes an EXPLAIN on the request
 * (which doubles as a syntax check)
 * and when EXPLAIN successful, sends the request to one of
 * three classes of worker.
 */
func handleApi(db *sql.DB, slow chan<- WorkItem, medium chan<- WorkItem, quick chan<- WorkItem, writer http.ResponseWriter, r *http.Request) {

	var res string
	var id int

	// create channel we want to receive the response on
	rchan := make(chan SqlResponse)

	writer.Header().Set("Access-Control-Allow-Origin", "*")
	writer.Header().Set("Content-Type", "application/json")

	// process GET/POST parameters
	r.ParseForm()
	tData := r.Form["data"]
	if tData == nil {
		log.Printf("no data field given\n")
		http.Error(writer, "no data field given", http.StatusBadRequest)
		return
	}
	data := tData[0]

	geojson := true
	tGeojson := r.Form["options[geojson]"]
	if tGeojson != nil {
		geojson, _ = strconv.ParseBool(tGeojson[0])
	}

	pretty := true
	tPretty := r.Form["options[pretty]"]
	if tPretty != nil {
		pretty, _ = strconv.ParseBool(tPretty[0])
	}

	collection := true
	tCollection := r.Form["options[collection]"]
	if tCollection != nil {
		collection, _ = strconv.ParseBool(tCollection[0])
	}

	countMutex.Lock()
	count++
	id = count
	countMutex.Unlock()

	log.Printf("request #%d: query '%s'\n", id,
		strings.Join(strings.Fields(strings.TrimSpace(data)), " "))

	var startTime = time.Now().UnixMilli()

	// yes there is a possible SQL injection here but risk mitigation
	// must be done on PostgreSQL side - we do not want to build an SQL parser
	rows, err := db.Query(fmt.Sprintf("EXPLAIN (%s)", data))

	if err != nil {
		log.Printf("request #%d: error in EXPLAIN: '%s'\n", id, err.Error())
		http.Error(writer, err.Error(), http.StatusBadRequest)
		return
	}

	// read only one row of EXPLAIN result
	rows.Next()

	// read only one column
	err = rows.Scan(&res)

	// discard query
	rows.Close()

	if err != nil {
		// in case EXPLAIN suddenly returns more columns
		log.Printf("request #%d: error in EXPLAIN: '%s'\n", id, err.Error())
		http.Error(writer, err.Error(), http.StatusBadRequest)
		return
	}

	// parse cost from EXPLAIN result
	rx, _ := regexp.Compile("cost=(\\d+\\.\\d+)\\.\\.(\\d+\\.\\d+) rows")
	cost := rx.FindStringSubmatch(res)
	if len(cost) != 3 {
		// EXPLAIN response not parseable
		log.Printf("request #%d: error in EXPLAIN: '%s'\n", id, err.Error())
		http.Error(writer, err.Error(), http.StatusBadRequest)
		return
	}

	// log.Printf("cost from %s to %s", cost[1], cost[2])
	from, err := strconv.ParseFloat(cost[1], 10)
	if err != nil {
		log.Printf("request #%d: error in EXPLAIN: '%s'\n", id, err.Error())
		http.Error(writer, err.Error(), http.StatusBadRequest)
		return
	}
	to, err := strconv.ParseFloat(cost[2], 10)
	if err != nil {
		log.Printf("request #%d: error in EXPLAIN: '%s'\n", id, err.Error())
		http.Error(writer, err.Error(), http.StatusBadRequest)
		return
	}

	// use average of two cost values given by EXPLAIN
	med := int((from + to) / 2)

	// create work item...
	work := WorkItem{
		request:    data,
		pretty:     pretty,
		geojson:    geojson,
		collection: collection,
		response:   rchan}

	// ... and send to appropriate channel
	if med < QuickMediumThreshold {
		log.Printf("request #%d: medium cost is %d, sending to quick worker\n", id, med)
		quick <- work
	} else if med < MediumSlowThreshold {
		log.Printf("request #%d: medium cost is %d, sending to medium worker\n", id, med)
		medium <- work
	} else {
		log.Printf("request #%d: medium cost is %d, sending to slow worker\n", id, med)
		slow <- work
	}

	// wait for response
	rv := <-rchan

	var elapsed = time.Now().UnixMilli() - startTime

	// and send response to HTTP client
	if rv.err {
		// FIXME it isn't really a bad request if it fails here, is it?
		log.Printf("request #%d: error from database after %dms: '%s'\n",
			id, elapsed, rv.result)
		http.Error(writer, rv.result, http.StatusBadRequest)
	}

	log.Printf("request #%d: completed after %dms, response size is %d\n",
		id, elapsed, len(rv.result))
	fmt.Fprintf(writer, "%s", rv.result)
}

/*
 * main program
 */
func main() {

	// don't log timestamp since systemd already does
	log.SetFlags(0)

	// open a connection to the database
	connStr := fmt.Sprintf("host=%s port=%d user=%s password=%s dbname=%s sslmode=disable options='-c statement_timeout=36000000'",
		Host, Port, User, Password, DBName)
	db, err := sql.Open("postgres", connStr)
	if err != nil {
		log.Fatal(err)
	}
	defer db.Close()

	db.SetMaxIdleConns(100)
	db.SetMaxOpenConns(200)
	db.SetConnMaxLifetime(time.Hour)

	// verify the connection
	err = db.Ping()
	if err != nil {
		log.Fatal(err)
	}

	// initialize goroutines
	quick_jobs := make(chan WorkItem, 50)
	for w := 1; w <= 10; w++ {
		go worker(db, 100+w, quick_jobs)
	}
	medium_jobs := make(chan WorkItem, 50)
	for w := 1; w <= 4; w++ {
		go worker(db, 200+w, medium_jobs)
	}
	slow_jobs := make(chan WorkItem, 50)
	for w := 1; w <= 2; w++ {
		go worker(db, 300+w, slow_jobs)
	}

	// set up a ticker to log how many busy workers there are
	ticker := time.NewTicker(30 * time.Second)
	go func() {
		for {
			<-ticker.C
			log.Printf("idle workers: %d/10 quick, %d/4 medium, %d/2 slow; request count: %d\n",
				idle[1], idle[2], idle[3], count)
		}
	}()

	// set up callback for /interpreter URL
	http.HandleFunc("/interpreter", func(w http.ResponseWriter, r *http.Request) {
		handleApi(db, slow_jobs, medium_jobs, quick_jobs, w, r)
	})

	// endless loop
	log.Fatal(http.ListenAndServe(fmt.Sprintf(":%d", ListenPort), nil))

}
